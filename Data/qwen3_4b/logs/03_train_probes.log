============================================================
Script: train_sycophancy_probe.py
Model: qwen3_4b
Started: 2025-11-26T15:57:06.836781
Full dataset: True
============================================================

[2025-11-26 16:01:34] Loading hidden states and LLM judge labels...
[2025-11-26 16:01:34] Hidden states shape: (4000, 37, 2560)
[2025-11-26 16:01:34] Model: 36 layers, hidden_dim=2560
[2025-11-26 16:01:34] Extraction methods: ['last_prompt', 'last_response', 'mean_response']
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] Total examples: 4000
[2025-11-26 16:01:34] Sycophantic (LLM judge): 582 (14.6%)
[2025-11-26 16:01:34] Not sycophantic: 3418
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] TRAINING PROBES: LAST_PROMPT
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34]   embed: train=85.44%, test=85.50%
[2025-11-26 16:01:34]   layer_1: train=86.41%, test=86.00%
[2025-11-26 16:01:34]   layer_2: train=86.16%, test=85.75%
[2025-11-26 16:01:34]   layer_3: train=85.78%, test=84.75%
[2025-11-26 16:01:34]   layer_4: train=85.66%, test=85.25%
[2025-11-26 16:01:34]   layer_5: train=85.56%, test=85.75%
[2025-11-26 16:01:34]   layer_6: train=85.34%, test=85.50%
[2025-11-26 16:01:34]   layer_7: train=85.47%, test=85.50%
[2025-11-26 16:01:34]   layer_8: train=85.78%, test=85.50%
[2025-11-26 16:01:34]   layer_9: train=85.84%, test=85.50%
[2025-11-26 16:01:34]   layer_10: train=86.09%, test=85.50%
[2025-11-26 16:01:34]   layer_11: train=85.75%, test=86.12%
[2025-11-26 16:01:34]   layer_12: train=85.72%, test=85.62%
[2025-11-26 16:01:34]   layer_13: train=85.34%, test=85.50%
[2025-11-26 16:01:34]   layer_14: train=85.47%, test=85.62%
[2025-11-26 16:01:34]   layer_15: train=85.62%, test=84.88%
[2025-11-26 16:01:34]   layer_16: train=85.44%, test=85.25%
[2025-11-26 16:01:34]   layer_17: train=85.47%, test=85.38%
[2025-11-26 16:01:34]   layer_18: train=85.50%, test=86.38%
[2025-11-26 16:01:34]   layer_19: train=85.59%, test=86.25%
[2025-11-26 16:01:34]   layer_20: train=85.78%, test=86.00%
[2025-11-26 16:01:34]   layer_21: train=85.91%, test=85.88%
[2025-11-26 16:01:34]   layer_22: train=86.06%, test=86.38%
[2025-11-26 16:01:34]   layer_23: train=86.22%, test=86.25%
[2025-11-26 16:01:34]   layer_24: train=86.62%, test=87.00%
[2025-11-26 16:01:34]   layer_25: train=86.19%, test=85.88%
[2025-11-26 16:01:34]   layer_26: train=86.41%, test=85.50%
[2025-11-26 16:01:34]   layer_27: train=86.50%, test=84.88%
[2025-11-26 16:01:34]   layer_28: train=86.53%, test=86.12%
[2025-11-26 16:01:34]   layer_29: train=86.31%, test=86.75%
[2025-11-26 16:01:34]   layer_30: train=86.19%, test=86.00%
[2025-11-26 16:01:34]   layer_31: train=85.53%, test=86.62%
[2025-11-26 16:01:34]   layer_32: train=86.03%, test=85.88%
[2025-11-26 16:01:34]   layer_33: train=86.00%, test=85.50%
[2025-11-26 16:01:34]   layer_34: train=86.03%, test=85.62%
[2025-11-26 16:01:34]   layer_35: train=86.00%, test=85.75%
[2025-11-26 16:01:34]   layer_36: train=86.22%, test=86.38%
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] *** BEST LAYER for last_prompt: 24 (test acc: 87.00%) ***
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] TRAINING PROBES: LAST_RESPONSE
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34]   embed: train=85.53%, test=85.38%
[2025-11-26 16:01:34]   layer_1: train=85.59%, test=84.75%
[2025-11-26 16:01:34]   layer_2: train=85.50%, test=85.25%
[2025-11-26 16:01:34]   layer_3: train=85.62%, test=85.25%
[2025-11-26 16:01:34]   layer_4: train=85.53%, test=84.88%
[2025-11-26 16:01:34]   layer_5: train=85.66%, test=85.00%
[2025-11-26 16:01:34]   layer_6: train=85.59%, test=85.38%
[2025-11-26 16:01:34]   layer_7: train=85.66%, test=85.12%
[2025-11-26 16:01:34]   layer_8: train=85.41%, test=85.38%
[2025-11-26 16:01:34]   layer_9: train=85.62%, test=84.75%
[2025-11-26 16:01:34]   layer_10: train=85.69%, test=85.25%
[2025-11-26 16:01:34]   layer_11: train=85.62%, test=85.25%
[2025-11-26 16:01:34]   layer_12: train=85.56%, test=85.50%
[2025-11-26 16:01:34]   layer_13: train=85.72%, test=85.50%
[2025-11-26 16:01:34]   layer_14: train=85.44%, test=85.25%
[2025-11-26 16:01:34]   layer_15: train=85.47%, test=85.38%
[2025-11-26 16:01:34]   layer_16: train=85.50%, test=85.25%
[2025-11-26 16:01:34]   layer_17: train=85.47%, test=85.50%
[2025-11-26 16:01:34]   layer_18: train=85.53%, test=85.25%
[2025-11-26 16:01:34]   layer_19: train=85.59%, test=85.50%
[2025-11-26 16:01:34]   layer_20: train=85.78%, test=85.50%
[2025-11-26 16:01:34]   layer_21: train=85.56%, test=85.38%
[2025-11-26 16:01:34]   layer_22: train=85.72%, test=84.50%
[2025-11-26 16:01:34]   layer_23: train=85.62%, test=85.38%
[2025-11-26 16:01:34]   layer_24: train=85.53%, test=85.12%
[2025-11-26 16:01:34]   layer_25: train=85.69%, test=85.25%
[2025-11-26 16:01:34]   layer_26: train=85.78%, test=85.62%
[2025-11-26 16:01:34]   layer_27: train=85.88%, test=86.00%
[2025-11-26 16:01:34]   layer_28: train=85.72%, test=85.75%
[2025-11-26 16:01:34]   layer_29: train=85.50%, test=85.75%
[2025-11-26 16:01:34]   layer_30: train=85.53%, test=85.50%
[2025-11-26 16:01:34]   layer_31: train=85.53%, test=85.62%
[2025-11-26 16:01:34]   layer_32: train=85.53%, test=85.50%
[2025-11-26 16:01:34]   layer_33: train=85.75%, test=85.38%
[2025-11-26 16:01:34]   layer_34: train=85.47%, test=85.12%
[2025-11-26 16:01:34]   layer_35: train=85.78%, test=84.75%
[2025-11-26 16:01:34]   layer_36: train=85.44%, test=85.50%
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] *** BEST LAYER for last_response: 27 (test acc: 86.00%) ***
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] TRAINING PROBES: MEAN_RESPONSE
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34]   embed: train=85.84%, test=85.50%
[2025-11-26 16:01:34]   layer_1: train=85.88%, test=86.00%
[2025-11-26 16:01:34]   layer_2: train=85.81%, test=85.38%
[2025-11-26 16:01:34]   layer_3: train=85.59%, test=85.38%
[2025-11-26 16:01:34]   layer_4: train=85.84%, test=85.12%
[2025-11-26 16:01:34]   layer_5: train=86.09%, test=86.25%
[2025-11-26 16:01:34]   layer_6: train=85.97%, test=85.88%
[2025-11-26 16:01:34]   layer_7: train=85.88%, test=85.25%
[2025-11-26 16:01:34]   layer_8: train=86.22%, test=85.75%
[2025-11-26 16:01:34]   layer_9: train=85.62%, test=86.62%
[2025-11-26 16:01:34]   layer_10: train=85.75%, test=85.88%
[2025-11-26 16:01:34]   layer_11: train=85.75%, test=85.88%
[2025-11-26 16:01:34]   layer_12: train=85.94%, test=85.25%
[2025-11-26 16:01:34]   layer_13: train=86.00%, test=85.88%
[2025-11-26 16:01:34]   layer_14: train=86.03%, test=86.50%
[2025-11-26 16:01:34]   layer_15: train=86.06%, test=86.00%
[2025-11-26 16:01:34]   layer_16: train=86.03%, test=87.00%
[2025-11-26 16:01:34]   layer_17: train=86.28%, test=87.00%
[2025-11-26 16:01:34]   layer_18: train=86.38%, test=86.62%
[2025-11-26 16:01:34]   layer_19: train=86.78%, test=86.12%
[2025-11-26 16:01:34]   layer_20: train=86.72%, test=87.25%
[2025-11-26 16:01:34]   layer_21: train=87.19%, test=87.50%
[2025-11-26 16:01:34]   layer_22: train=86.81%, test=88.38%
[2025-11-26 16:01:34]   layer_23: train=86.66%, test=87.50%
[2025-11-26 16:01:34]   layer_24: train=86.25%, test=87.12%
[2025-11-26 16:01:34]   layer_25: train=86.25%, test=86.88%
[2025-11-26 16:01:34]   layer_26: train=86.47%, test=86.25%
[2025-11-26 16:01:34]   layer_27: train=86.62%, test=85.62%
[2025-11-26 16:01:34]   layer_28: train=86.44%, test=85.62%
[2025-11-26 16:01:34]   layer_29: train=86.38%, test=85.75%
[2025-11-26 16:01:34]   layer_30: train=86.28%, test=85.88%
[2025-11-26 16:01:34]   layer_31: train=86.19%, test=86.12%
[2025-11-26 16:01:34]   layer_32: train=86.09%, test=86.88%
[2025-11-26 16:01:34]   layer_33: train=86.12%, test=86.75%
[2025-11-26 16:01:34]   layer_34: train=86.16%, test=86.88%
[2025-11-26 16:01:34]   layer_35: train=86.00%, test=87.25%
[2025-11-26 16:01:34]   layer_36: train=86.28%, test=86.12%
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] *** BEST LAYER for mean_response: 22 (test acc: 88.38%) ***
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] COMPARISON OF EXTRACTION METHODS
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] Method               Best Layer   Test Acc    
[2025-11-26 16:01:34] --------------------------------------------
[2025-11-26 16:01:34] last_prompt          24           87.00%      
[2025-11-26 16:01:34] last_response        27           86.00%      
[2025-11-26 16:01:34] mean_response        22           88.38%      
[2025-11-26 16:01:34] --------------------------------------------
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] *** BEST OVERALL: mean_response (layer 22, acc: 88.38%) ***
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] SPARSE PROBE ANALYSIS (mean_response, Layer 22)
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] Non-zero weights: 479/2560 (18.7%)
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] Top 10 neurons PREDICTING sycophancy:
[2025-11-26 16:01:34]   1. Neuron 2065: weight = 0.4137
[2025-11-26 16:01:34]   2. Neuron 2546: weight = 0.3399
[2025-11-26 16:01:34]   3. Neuron 2287: weight = 0.2632
[2025-11-26 16:01:34]   4. Neuron 1586: weight = 0.2607
[2025-11-26 16:01:34]   5. Neuron 2530: weight = 0.2551
[2025-11-26 16:01:34]   6. Neuron 1465: weight = 0.2339
[2025-11-26 16:01:34]   7. Neuron 2014: weight = 0.2179
[2025-11-26 16:01:34]   8. Neuron 2338: weight = 0.2060
[2025-11-26 16:01:34]   9. Neuron 1289: weight = 0.2060
[2025-11-26 16:01:34]   10. Neuron 210: weight = 0.1807
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] Top 10 neurons AGAINST sycophancy:
[2025-11-26 16:01:34]   1. Neuron 1794: weight = -0.2517
[2025-11-26 16:01:34]   2. Neuron 2354: weight = -0.2372
[2025-11-26 16:01:34]   3. Neuron 1056: weight = -0.2311
[2025-11-26 16:01:34]   4. Neuron 1918: weight = -0.2006
[2025-11-26 16:01:34]   5. Neuron 1819: weight = -0.1910
[2025-11-26 16:01:34]   6. Neuron 2244: weight = -0.1888
[2025-11-26 16:01:34]   7. Neuron 1509: weight = -0.1810
[2025-11-26 16:01:34]   8. Neuron 1267: weight = -0.1792
[2025-11-26 16:01:34]   9. Neuron 427: weight = -0.1788
[2025-11-26 16:01:34]   10. Neuron 992: weight = -0.1745
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] SAVING PROBES
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] All probes saved to /root/Syco-Sup/results/qwen3_4b/sycophancy_probes.pkl
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] DONE!
[2025-11-26 16:01:34] ============================================================
[2025-11-26 16:01:34] 
[2025-11-26 16:01:34] Next step: python test_probe_on_eval.py

============================================================
Finished: 2025-11-26T16:01:34.667873
Exit code: 0
============================================================
